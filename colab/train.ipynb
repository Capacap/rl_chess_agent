{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Chess RL Agent - Training (Optimized)\n\n**New:** Shaped rewards + speed optimizations (~60% faster, more reliable convergence)\n\n**Strategy:** Split into safe 3-4 hour sessions (no browser babysitting needed)\n\n**Workflow:**\n1. Run cells 1-6 once â†’ Auto-trains or resumes (~3-4 hours per session)\n2. Training completes, checkpoints auto-backup to Drive\n3. Close browser, take a break\n4. Next session: Re-run cells 1-6 â†’ Automatically resumes where you left off\n\n**Total time:** 2-3 sessions Ã— 3-4 hours = 8-12 hours for 10 iterations\n\n**What's optimized:**\n- Shaped rewards (material + pawn advancement + activity) prevent draw loops\n- Smaller network (48 channels, 3 blocks) for faster inference\n- Asymmetric MCTS (12 sims self-play, 40 arena) saves time without sacrificing quality\n- Adaptive schedule (progressive games/epochs/lr) optimizes learning\n- Early stopping skips wasted epochs"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Verify GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"\\nâš  Go to Runtime â†’ Change runtime type â†’ Select GPU (L4 recommended)\")\n",
    "else:\n",
    "    print(\"\\nâœ“ GPU ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "!mkdir -p /content/drive/MyDrive/chess_checkpoints\n",
    "\n",
    "print(\"âœ“ Drive mounted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf rl_chess_agent\n",
    "!git clone https://github.com/capacap/rl_chess_agent.git\n",
    "%cd rl_chess_agent\n",
    "\n",
    "print(\"âœ“ Repository cloned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 4. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install chess library (Colab has torch, numpy, etc.)\n",
    "!pip install -q -r requirements-colab.txt\n",
    "\n",
    "# Verify imports\n",
    "import chess\n",
    "from model.network import ChessNet\n",
    "\n",
    "print(f\"âœ“ Dependencies installed\")\n",
    "print(f\"  chess: {chess.__version__}\")\n",
    "print(f\"  torch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 5. Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "# === Project Configuration ===\n# Set this once and keep it the same across all sessions\nPROJECT_NAME = \"proof_of_concept\"  # Change this for different training runs\n\n# === Session Configuration ===\nSESSION_ITERATIONS = 5    # Iterations per session (3-4 hours each with optimizations)\nTOTAL_TARGET = 10         # Total iterations you want overall\n\n# === Optimized Training Parameters ===\n# These defaults use shaped rewards + speed optimizations (60% faster!)\n\n# Network (smaller = faster)\nCHANNELS = 48             # Down from 64 (~500K params, fast inference)\nBLOCKS = 3                # Down from 4\n\n# MCTS (asymmetric: fewer sims in self-play, more in arena)\nSIMULATIONS = 12          # Self-play sims (down from 20)\nSIMULATIONS_ARENA = 40    # Arena sims (keep high for evaluation)\n\n# Schedule (adaptive by default, progressively more games/epochs)\nUSE_ADAPTIVE = True       # Use progressive schedule from config.py\nARENA_GAMES = 16          # Down from 20 (adjusted threshold for statistical significance)\n\n# Advanced (only change if disabling adaptive schedule)\nBATCH_SIZE = 256          # Ignored if adaptive schedule enabled\nEPOCHS = 5                # Ignored if adaptive schedule enabled  \nLEARNING_RATE = 1e-3      # Ignored if adaptive schedule enabled\n\n# Checkpoint directories\nCHECKPOINT_DIR = f\"checkpoints/{PROJECT_NAME}\"\nGDRIVE_BACKUP = \"/content/drive/MyDrive/chess_checkpoints\"\n\nprint(\"Training Configuration (Optimized):\")\nprint(f\"  Project: {PROJECT_NAME}\")\nprint(f\"  Target: {TOTAL_TARGET} iterations total\")\nprint(f\"  Session size: {SESSION_ITERATIONS} iterations (~{SESSION_ITERATIONS * 0.8:.0f}-{SESSION_ITERATIONS * 1.3:.0f} hours)\")\nprint(f\"\\nOptimizations enabled:\")\nprint(f\"  - Shaped rewards (prevents draw loops)\")\nprint(f\"  - Smaller network ({CHANNELS} channels, {BLOCKS} blocks)\")\nprint(f\"  - Asymmetric MCTS ({SIMULATIONS} sims self-play, {SIMULATIONS_ARENA} arena)\")\nprint(f\"  - Adaptive schedule (progressive games/epochs/lr)\")\nprint(f\"  - Early stopping (skip wasted epochs)\")\nprint(f\"\\nExpected: ~60% faster than baseline, more reliable convergence\")\nprint(f\"\\nCheckpoints: {CHECKPOINT_DIR}\")\nprint(f\"Drive backup: {GDRIVE_BACKUP}/{PROJECT_NAME}/\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 6. Train (auto-resumes if checkpoint exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport glob\n\n# Check for existing checkpoints\nexisting_checkpoints = sorted(glob.glob(f\"{CHECKPOINT_DIR}/iteration_*.pt\"))\n\nif existing_checkpoints:\n    # Resume from latest checkpoint\n    latest = existing_checkpoints[-1]\n    completed = int(latest.split('_')[-1].split('.')[0])\n    remaining = TOTAL_TARGET - completed\n    \n    print(\"=\"*60)\n    print(\"RESUMING TRAINING (Optimized)\")\n    print(\"=\"*60)\n    print(f\"  Progress: {completed}/{TOTAL_TARGET} iterations complete\")\n    print(f\"  Resuming from: {os.path.basename(latest)}\")\n    \n    if remaining > 0:\n        iterations_this_session = min(SESSION_ITERATIONS, remaining)\n        print(f\"  This session: {iterations_this_session} iterations (~{iterations_this_session * 0.8:.0f}-{iterations_this_session * 1.3:.0f} hours)\")\n        print(f\"  Remaining after: {remaining - iterations_this_session} iterations\\n\")\n        \n        # Resume training with optimized parameters\n        if USE_ADAPTIVE:\n            !python train.py \\\n              --resume {latest} \\\n              --iterations {iterations_this_session} \\\n              --simulations {SIMULATIONS} \\\n              --simulations-arena {SIMULATIONS_ARENA} \\\n              --channels {CHANNELS} \\\n              --blocks {BLOCKS} \\\n              --checkpoint-dir {CHECKPOINT_DIR} \\\n              --gdrive-backup-dir {GDRIVE_BACKUP}\n        else:\n            !python train.py \\\n              --resume {latest} \\\n              --iterations {iterations_this_session} \\\n              --simulations {SIMULATIONS} \\\n              --simulations-arena {SIMULATIONS_ARENA} \\\n              --channels {CHANNELS} \\\n              --blocks {BLOCKS} \\\n              --games-per-iter {GAMES_PER_ITER} \\\n              --arena-games {ARENA_GAMES} \\\n              --batch-size {BATCH_SIZE} \\\n              --epochs {EPOCHS} \\\n              --lr {LEARNING_RATE} \\\n              --no-adaptive-schedule \\\n              --checkpoint-dir {CHECKPOINT_DIR} \\\n              --gdrive-backup-dir {GDRIVE_BACKUP}\n        \n        new_completed = completed + iterations_this_session\n        print(f\"\\n{'='*60}\")\n        print(f\"SESSION COMPLETE\")\n        print(f\"{'='*60}\")\n        print(f\"  Progress: {new_completed}/{TOTAL_TARGET} iterations\")\n        print(f\"  Checkpoints: {GDRIVE_BACKUP}/{PROJECT_NAME}/\")\n        \n        if new_completed >= TOTAL_TARGET:\n            print(f\"\\nðŸŽ‰ TRAINING COMPLETE!\")\n            print(f\"  Final model: {CHECKPOINT_DIR}/iteration_{TOTAL_TARGET}.pkl\")\n        else:\n            print(f\"\\nNext steps:\")\n            print(f\"  1. Close browser (checkpoints saved to Drive)\")\n            print(f\"  2. When ready: Re-run cells 1-6 to continue\")\n            print(f\"  3. Remaining: {TOTAL_TARGET - new_completed} iterations\")\n    else:\n        print(f\"\\nðŸŽ‰ TRAINING ALREADY COMPLETE!\")\n        print(f\"  Final model: {CHECKPOINT_DIR}/iteration_{TOTAL_TARGET}.pkl\")\n        print(f\"  Download from: {GDRIVE_BACKUP}/{PROJECT_NAME}/\")\n\nelse:\n    # Start fresh training\n    print(\"=\"*60)\n    print(\"STARTING NEW TRAINING (Optimized)\")\n    print(\"=\"*60)\n    print(f\"  Project: {PROJECT_NAME}\")\n    print(f\"  This session: {SESSION_ITERATIONS} iterations (~{SESSION_ITERATIONS * 0.8:.0f}-{SESSION_ITERATIONS * 1.3:.0f} hours)\")\n    print(f\"  Total target: {TOTAL_TARGET} iterations\\n\")\n    \n    # Start training with optimized parameters\n    if USE_ADAPTIVE:\n        !python train.py \\\n          --iterations {SESSION_ITERATIONS} \\\n          --simulations {SIMULATIONS} \\\n          --simulations-arena {SIMULATIONS_ARENA} \\\n          --channels {CHANNELS} \\\n          --blocks {BLOCKS} \\\n          --checkpoint-dir {CHECKPOINT_DIR} \\\n          --gdrive-backup-dir {GDRIVE_BACKUP}\n    else:\n        !python train.py \\\n          --iterations {SESSION_ITERATIONS} \\\n          --simulations {SIMULATIONS} \\\n          --simulations-arena {SIMULATIONS_ARENA} \\\n          --channels {CHANNELS} \\\n          --blocks {BLOCKS} \\\n          --games-per-iter {GAMES_PER_ITER} \\\n          --arena-games {ARENA_GAMES} \\\n          --batch-size {BATCH_SIZE} \\\n          --epochs {EPOCHS} \\\n          --lr {LEARNING_RATE} \\\n          --no-adaptive-schedule \\\n          --checkpoint-dir {CHECKPOINT_DIR} \\\n          --gdrive-backup-dir {GDRIVE_BACKUP}\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"SESSION COMPLETE\")\n    print(f\"{'='*60}\")\n    print(f\"  Progress: {SESSION_ITERATIONS}/{TOTAL_TARGET} iterations\")\n    print(f\"  Checkpoints: {GDRIVE_BACKUP}/{PROJECT_NAME}/\")\n    \n    if SESSION_ITERATIONS >= TOTAL_TARGET:\n        print(f\"\\nðŸŽ‰ TRAINING COMPLETE!\")\n        print(f\"  Final model: {CHECKPOINT_DIR}/iteration_{TOTAL_TARGET}.pkl\")\n    else:\n        print(f\"\\nNext steps:\")\n        print(f\"  1. Close browser (checkpoints saved to Drive)\")\n        print(f\"  2. When ready: Re-run cells 1-6 to continue\")\n        print(f\"  3. Remaining: {TOTAL_TARGET - SESSION_ITERATIONS} iterations\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": "## Troubleshooting\n\n**Out of memory:** \n- Reduce `CHANNELS = 32` or `BLOCKS = 2` (smaller network)\n- Or disable adaptive schedule and set `BATCH_SIZE = 128`\n\n**Too slow:** \n- Reduce `SIMULATIONS = 10` (self-play) or `SIMULATIONS_ARENA = 30` (arena)\n- Or set `USE_ADAPTIVE = False` and reduce `GAMES_PER_ITER`\n\n**Disable optimizations:**\n- Set `USE_ADAPTIVE = False` to use fixed parameters\n- Set `CHANNELS = 64, BLOCKS = 4` for original network size\n- Set `SIMULATIONS = SIMULATIONS_ARENA = 20` for symmetric MCTS\n\n**Start over:** Change `PROJECT_NAME` in cell 5 to create a new training run\n\n**Download checkpoints:** Already in Google Drive at `/MyDrive/chess_checkpoints/{PROJECT_NAME}/`\n\n**Check progress:** Look at `training.log` in checkpoints for detailed metrics"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}